{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport string\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\n\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model\n\nprint(os.listdir(\"../input\"))\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', -1)\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-28T10:24:39.395381Z","iopub.execute_input":"2022-07-28T10:24:39.396430Z","iopub.status.idle":"2022-07-28T10:24:45.953618Z","shell.execute_reply.started":"2022-07-28T10:24:39.395790Z","shell.execute_reply":"2022-07-28T10:24:45.951838Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"lines=pd.read_csv(\"../input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\",encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:25:22.934724Z","iopub.execute_input":"2022-07-28T10:25:22.935195Z","iopub.status.idle":"2022-07-28T10:25:24.181267Z","shell.execute_reply.started":"2022-07-28T10:25:22.935166Z","shell.execute_reply":"2022-07-28T10:25:24.180053Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"lines['source'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:25:34.289770Z","iopub.execute_input":"2022-07-28T10:25:34.290127Z","iopub.status.idle":"2022-07-28T10:25:34.329676Z","shell.execute_reply.started":"2022-07-28T10:25:34.290098Z","shell.execute_reply":"2022-07-28T10:25:34.328395Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"lines=lines[lines['source']=='ted']","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:25:43.759962Z","iopub.execute_input":"2022-07-28T10:25:43.760400Z","iopub.status.idle":"2022-07-28T10:25:43.802233Z","shell.execute_reply.started":"2022-07-28T10:25:43.760354Z","shell.execute_reply":"2022-07-28T10:25:43.801107Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"lines.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:26:03.118706Z","iopub.execute_input":"2022-07-28T10:26:03.119065Z","iopub.status.idle":"2022-07-28T10:26:03.142174Z","shell.execute_reply.started":"2022-07-28T10:26:03.119037Z","shell.execute_reply":"2022-07-28T10:26:03.140928Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"pd.isnull(lines).sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:26:16.504102Z","iopub.execute_input":"2022-07-28T10:26:16.505072Z","iopub.status.idle":"2022-07-28T10:26:16.528045Z","shell.execute_reply.started":"2022-07-28T10:26:16.505026Z","shell.execute_reply":"2022-07-28T10:26:16.526789Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"lines=lines[~pd.isnull(lines['hindi_sentence'])]","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:26:24.599404Z","iopub.execute_input":"2022-07-28T10:26:24.599803Z","iopub.status.idle":"2022-07-28T10:26:24.617160Z","shell.execute_reply.started":"2022-07-28T10:26:24.599774Z","shell.execute_reply":"2022-07-28T10:26:24.615889Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"lines.drop_duplicates(inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:26:32.319417Z","iopub.execute_input":"2022-07-28T10:26:32.319991Z","iopub.status.idle":"2022-07-28T10:26:32.372253Z","shell.execute_reply.started":"2022-07-28T10:26:32.319960Z","shell.execute_reply":"2022-07-28T10:26:32.371131Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"lines=lines.sample(n=25000,random_state=42)\nlines.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:26:40.570486Z","iopub.execute_input":"2022-07-28T10:26:40.571155Z","iopub.status.idle":"2022-07-28T10:26:40.591968Z","shell.execute_reply.started":"2022-07-28T10:26:40.571110Z","shell.execute_reply":"2022-07-28T10:26:40.590815Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Lowercase all characters\nlines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.lower())\nlines['english_sentence']=lines['english_sentence'].apply(lambda x: x.lower())","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:26:48.460447Z","iopub.execute_input":"2022-07-28T10:26:48.461010Z","iopub.status.idle":"2022-07-28T10:26:48.519666Z","shell.execute_reply.started":"2022-07-28T10:26:48.460970Z","shell.execute_reply":"2022-07-28T10:26:48.518231Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Remove quotes\nlines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))\nlines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:26:55.044796Z","iopub.execute_input":"2022-07-28T10:26:55.045166Z","iopub.status.idle":"2022-07-28T10:26:55.133031Z","shell.execute_reply.started":"2022-07-28T10:26:55.045137Z","shell.execute_reply":"2022-07-28T10:26:55.131563Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"exclude = set(string.punctuation) # Set of all special characters\n# Remove all the special characters\nlines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\nlines['english_sentence']=lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:27:02.809761Z","iopub.execute_input":"2022-07-28T10:27:02.810187Z","iopub.status.idle":"2022-07-28T10:27:03.125454Z","shell.execute_reply.started":"2022-07-28T10:27:02.810155Z","shell.execute_reply":"2022-07-28T10:27:03.124303Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Remove all numbers from text\nremove_digits = str.maketrans('', '', digits)\nlines['english_sentence']=lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\nlines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n\nlines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n\n# Remove extra spaces\nlines['english_sentence']=lines['english_sentence'].apply(lambda x: x.strip())\nlines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.strip())\nlines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\nlines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:27:09.194175Z","iopub.execute_input":"2022-07-28T10:27:09.194636Z","iopub.status.idle":"2022-07-28T10:27:09.664701Z","shell.execute_reply.started":"2022-07-28T10:27:09.194603Z","shell.execute_reply":"2022-07-28T10:27:09.663547Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Add start and end tokens to target sequences\nlines['english_sentence'] = lines['english_sentence'].apply(lambda x : 'START_ '+ x + ' _END')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:27:15.719625Z","iopub.execute_input":"2022-07-28T10:27:15.719975Z","iopub.status.idle":"2022-07-28T10:27:15.739558Z","shell.execute_reply.started":"2022-07-28T10:27:15.719947Z","shell.execute_reply":"2022-07-28T10:27:15.738437Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"lines.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:27:25.764464Z","iopub.execute_input":"2022-07-28T10:27:25.764928Z","iopub.status.idle":"2022-07-28T10:27:25.781219Z","shell.execute_reply.started":"2022-07-28T10:27:25.764901Z","shell.execute_reply":"2022-07-28T10:27:25.779829Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"### Get English and Hindi Vocabulary\nall_eng_words=set()\nfor eng in lines['english_sentence']:\n    for word in eng.split():\n        if word not in all_eng_words:\n            all_eng_words.add(word)\n\nall_hindi_words=set()\nfor hin in lines['hindi_sentence']:\n    for word in hin.split():\n        if word not in all_hindi_words:\n            all_hindi_words.add(word)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:27:36.370065Z","iopub.execute_input":"2022-07-28T10:27:36.370464Z","iopub.status.idle":"2022-07-28T10:27:36.501717Z","shell.execute_reply.started":"2022-07-28T10:27:36.370435Z","shell.execute_reply":"2022-07-28T10:27:36.500587Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"len(all_eng_words)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:27:42.323749Z","iopub.execute_input":"2022-07-28T10:27:42.324111Z","iopub.status.idle":"2022-07-28T10:27:42.332647Z","shell.execute_reply.started":"2022-07-28T10:27:42.324083Z","shell.execute_reply":"2022-07-28T10:27:42.331306Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"len(all_hindi_words)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:28:24.730728Z","iopub.execute_input":"2022-07-28T10:28:24.731121Z","iopub.status.idle":"2022-07-28T10:28:24.740348Z","shell.execute_reply.started":"2022-07-28T10:28:24.731089Z","shell.execute_reply":"2022-07-28T10:28:24.738591Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"lines['length_eng_sentence']=lines['english_sentence'].apply(lambda x:len(x.split(\" \")))\nlines['length_hin_sentence']=lines['hindi_sentence'].apply(lambda x:len(x.split(\" \")))","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:28:33.594486Z","iopub.execute_input":"2022-07-28T10:28:33.594897Z","iopub.status.idle":"2022-07-28T10:28:33.663717Z","shell.execute_reply.started":"2022-07-28T10:28:33.594862Z","shell.execute_reply":"2022-07-28T10:28:33.662563Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"lines.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:28:42.624497Z","iopub.execute_input":"2022-07-28T10:28:42.624875Z","iopub.status.idle":"2022-07-28T10:28:42.639513Z","shell.execute_reply.started":"2022-07-28T10:28:42.624847Z","shell.execute_reply":"2022-07-28T10:28:42.638073Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"lines[lines['length_hin_sentence']>30].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:28:52.548483Z","iopub.execute_input":"2022-07-28T10:28:52.548890Z","iopub.status.idle":"2022-07-28T10:28:52.558951Z","shell.execute_reply.started":"2022-07-28T10:28:52.548861Z","shell.execute_reply":"2022-07-28T10:28:52.557697Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"lines.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:29:01.694665Z","iopub.execute_input":"2022-07-28T10:29:01.695130Z","iopub.status.idle":"2022-07-28T10:29:01.710694Z","shell.execute_reply.started":"2022-07-28T10:29:01.695102Z","shell.execute_reply":"2022-07-28T10:29:01.709285Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(\"maximum length of Hindi Sentence \",max(lines['length_hin_sentence']))\nprint(\"maximum length of English Sentence \",max(lines['length_eng_sentence']))","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:29:10.679208Z","iopub.execute_input":"2022-07-28T10:29:10.679700Z","iopub.status.idle":"2022-07-28T10:29:10.696837Z","shell.execute_reply.started":"2022-07-28T10:29:10.679671Z","shell.execute_reply":"2022-07-28T10:29:10.695612Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"max_length_src=max(lines['length_hin_sentence'])\nmax_length_tar=max(lines['length_eng_sentence'])","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:29:21.569520Z","iopub.execute_input":"2022-07-28T10:29:21.569894Z","iopub.status.idle":"2022-07-28T10:29:21.583879Z","shell.execute_reply.started":"2022-07-28T10:29:21.569851Z","shell.execute_reply":"2022-07-28T10:29:21.582577Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"input_words = sorted(list(all_hindi_words))\ntarget_words = sorted(list(all_eng_words))\nnum_encoder_tokens = len(all_hindi_words)\nnum_decoder_tokens = len(all_eng_words)\nnum_encoder_tokens, num_decoder_tokens","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:29:29.613695Z","iopub.execute_input":"2022-07-28T10:29:29.614063Z","iopub.status.idle":"2022-07-28T10:29:29.643639Z","shell.execute_reply.started":"2022-07-28T10:29:29.614036Z","shell.execute_reply":"2022-07-28T10:29:29.642165Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"num_decoder_tokens += 1 #for zero padding","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:29:40.894027Z","iopub.execute_input":"2022-07-28T10:29:40.894497Z","iopub.status.idle":"2022-07-28T10:29:40.900734Z","shell.execute_reply.started":"2022-07-28T10:29:40.894452Z","shell.execute_reply":"2022-07-28T10:29:40.899358Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\ntarget_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:30:13.109195Z","iopub.execute_input":"2022-07-28T10:30:13.109615Z","iopub.status.idle":"2022-07-28T10:30:13.132453Z","shell.execute_reply.started":"2022-07-28T10:30:13.109588Z","shell.execute_reply":"2022-07-28T10:30:13.131396Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\nreverse_target_char_index = dict((i, word) for word, i in target_token_index.items())","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:30:20.645929Z","iopub.execute_input":"2022-07-28T10:30:20.646426Z","iopub.status.idle":"2022-07-28T10:30:20.661967Z","shell.execute_reply.started":"2022-07-28T10:30:20.646396Z","shell.execute_reply":"2022-07-28T10:30:20.660789Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"lines = shuffle(lines)\nlines.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:30:27.313395Z","iopub.execute_input":"2022-07-28T10:30:27.313844Z","iopub.status.idle":"2022-07-28T10:30:27.336548Z","shell.execute_reply.started":"2022-07-28T10:30:27.313812Z","shell.execute_reply":"2022-07-28T10:30:27.335212Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"X, y = lines['hindi_sentence'], lines['english_sentence']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:30:37.134970Z","iopub.execute_input":"2022-07-28T10:30:37.135538Z","iopub.status.idle":"2022-07-28T10:30:37.156237Z","shell.execute_reply.started":"2022-07-28T10:30:37.135487Z","shell.execute_reply":"2022-07-28T10:30:37.155046Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"X_train.to_pickle('X_train.pkl')\nX_test.to_pickle('X_test.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:30:47.784517Z","iopub.execute_input":"2022-07-28T10:30:47.784879Z","iopub.status.idle":"2022-07-28T10:30:47.811362Z","shell.execute_reply.started":"2022-07-28T10:30:47.784851Z","shell.execute_reply":"2022-07-28T10:30:47.810277Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:30:10.391981Z","iopub.execute_input":"2022-07-28T12:30:10.392473Z","iopub.status.idle":"2022-07-28T12:30:10.403610Z","shell.execute_reply.started":"2022-07-28T12:30:10.392441Z","shell.execute_reply":"2022-07-28T12:30:10.402102Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Encoder-Decoder Architecture","metadata":{}},{"cell_type":"code","source":"def generate_batch(X = X_train, y = y_train, batch_size = 128):\n    ''' Generate a batch of data '''\n    while True:\n        for j in range(0, len(X), batch_size):\n            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n                for t, word in enumerate(input_text.split()):\n                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n                for t, word in enumerate(target_text.split()):\n                    if t<len(target_text.split())-1:\n                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n                    if t>0:\n                        # decoder target sequence (one hot encoded)\n                        # does not include the START_ token\n                        # Offset by one timestep\n                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n            yield([encoder_input_data, decoder_input_data], decoder_target_data)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:30:59.568730Z","iopub.execute_input":"2022-07-28T10:30:59.569160Z","iopub.status.idle":"2022-07-28T10:30:59.583531Z","shell.execute_reply.started":"2022-07-28T10:30:59.569130Z","shell.execute_reply":"2022-07-28T10:30:59.581807Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"latent_dim=300","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:31:07.514301Z","iopub.execute_input":"2022-07-28T10:31:07.514713Z","iopub.status.idle":"2022-07-28T10:31:07.519805Z","shell.execute_reply.started":"2022-07-28T10:31:07.514686Z","shell.execute_reply":"2022-07-28T10:31:07.518550Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Encoder\nencoder_inputs = Input(shape=(None,))\nenc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\nencoder_lstm = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:34:26.954727Z","iopub.execute_input":"2022-07-28T10:34:26.955319Z","iopub.status.idle":"2022-07-28T10:34:31.487420Z","shell.execute_reply.started":"2022-07-28T10:34:26.955272Z","shell.execute_reply":"2022-07-28T10:34:31.486068Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\ndec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\ndec_emb = dec_emb_layer(decoder_inputs)\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(dec_emb,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:34:40.659828Z","iopub.execute_input":"2022-07-28T10:34:40.660194Z","iopub.status.idle":"2022-07-28T10:34:41.640623Z","shell.execute_reply.started":"2022-07-28T10:34:40.660165Z","shell.execute_reply":"2022-07-28T10:34:41.639427Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\ndec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\ndec_emb = dec_emb_layer(decoder_inputs)\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(dec_emb,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:36:14.259768Z","iopub.execute_input":"2022-07-28T10:36:14.260135Z","iopub.status.idle":"2022-07-28T10:36:15.419512Z","shell.execute_reply.started":"2022-07-28T10:36:14.260091Z","shell.execute_reply":"2022-07-28T10:36:15.418118Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:36:23.874316Z","iopub.execute_input":"2022-07-28T10:36:23.874717Z","iopub.status.idle":"2022-07-28T10:36:23.895265Z","shell.execute_reply.started":"2022-07-28T10:36:23.874689Z","shell.execute_reply":"2022-07-28T10:36:23.893568Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:36:31.753946Z","iopub.execute_input":"2022-07-28T10:36:31.754429Z","iopub.status.idle":"2022-07-28T10:36:31.776531Z","shell.execute_reply.started":"2022-07-28T10:36:31.754353Z","shell.execute_reply":"2022-07-28T10:36:31.774409Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_samples = len(X_train)\nval_samples = len(X_test)\nbatch_size = 128\nepochs = 100","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:36:59.378743Z","iopub.execute_input":"2022-07-28T10:36:59.379122Z","iopub.status.idle":"2022-07-28T10:36:59.384942Z","shell.execute_reply.started":"2022-07-28T10:36:59.379093Z","shell.execute_reply":"2022-07-28T10:36:59.383486Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n                    steps_per_epoch = train_samples//batch_size,\n                    epochs=epochs,\n                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n                    validation_steps = val_samples//batch_size)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T10:37:07.324383Z","iopub.execute_input":"2022-07-28T10:37:07.324773Z","iopub.status.idle":"2022-07-28T11:57:36.794135Z","shell.execute_reply.started":"2022-07-28T10:37:07.324745Z","shell.execute_reply":"2022-07-28T11:57:36.792753Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model.save_weights('nmt_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:15:50.748903Z","iopub.execute_input":"2022-07-28T12:15:50.749343Z","iopub.status.idle":"2022-07-28T12:15:50.885351Z","shell.execute_reply.started":"2022-07-28T12:15:50.749314Z","shell.execute_reply":"2022-07-28T12:15:50.884128Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model.save('nmt_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:17:37.324000Z","iopub.execute_input":"2022-07-28T12:17:37.324415Z","iopub.status.idle":"2022-07-28T12:17:37.540437Z","shell.execute_reply.started":"2022-07-28T12:17:37.324386Z","shell.execute_reply":"2022-07-28T12:17:37.539161Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Encode the input sequence to get the \"thought vectors\"\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:18:05.404237Z","iopub.execute_input":"2022-07-28T12:18:05.404738Z","iopub.status.idle":"2022-07-28T12:18:06.571930Z","shell.execute_reply.started":"2022-07-28T12:18:05.404709Z","shell.execute_reply":"2022-07-28T12:18:06.570760Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:35:37.607252Z","iopub.execute_input":"2022-07-28T12:35:37.607796Z","iopub.status.idle":"2022-07-28T12:35:37.669764Z","shell.execute_reply.started":"2022-07-28T12:35:37.607768Z","shell.execute_reply":"2022-07-28T12:35:37.668489Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:37:02.920163Z","iopub.execute_input":"2022-07-28T12:37:02.920782Z","iopub.status.idle":"2022-07-28T12:37:03.030957Z","shell.execute_reply.started":"2022-07-28T12:37:02.920738Z","shell.execute_reply":"2022-07-28T12:37:03.029504Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = target_token_index['START_']\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += ' '+sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '_END' or\n           len(decoded_sentence) > 50):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:18:13.469765Z","iopub.execute_input":"2022-07-28T12:18:13.470611Z","iopub.status.idle":"2022-07-28T12:18:13.487443Z","shell.execute_reply.started":"2022-07-28T12:18:13.470540Z","shell.execute_reply":"2022-07-28T12:18:13.485997Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train_gen = generate_batch(X_train, y_train, batch_size = 1)\nk=-1\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:18:23.704098Z","iopub.execute_input":"2022-07-28T12:18:23.704800Z","iopub.status.idle":"2022-07-28T12:18:23.710476Z","shell.execute_reply.started":"2022-07-28T12:18:23.704768Z","shell.execute_reply":"2022-07-28T12:18:23.708953Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input Hindi sentence:', X_train[k:k+1].values[0])\nprint('Actual English Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted English Translation:', decoded_sentence[:-4])","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:19:27.344093Z","iopub.execute_input":"2022-07-28T12:19:27.344507Z","iopub.status.idle":"2022-07-28T12:19:27.912159Z","shell.execute_reply.started":"2022-07-28T12:19:27.344477Z","shell.execute_reply":"2022-07-28T12:19:27.910977Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"(input_seq, actual_output), _ = next(train_gen)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:28:41.503209Z","iopub.execute_input":"2022-07-28T12:28:41.503946Z","iopub.status.idle":"2022-07-28T12:28:41.511194Z","shell.execute_reply.started":"2022-07-28T12:28:41.503911Z","shell.execute_reply":"2022-07-28T12:28:41.509697Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"input_seq","metadata":{"execution":{"iopub.status.busy":"2022-07-28T12:28:48.553070Z","iopub.execute_input":"2022-07-28T12:28:48.553658Z","iopub.status.idle":"2022-07-28T12:28:48.563940Z","shell.execute_reply.started":"2022-07-28T12:28:48.553624Z","shell.execute_reply":"2022-07-28T12:28:48.562333Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input Hindi sentence:', X_train[k:k+1].values[0])\nprint('Actual English Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted English Translation:', decoded_sentence[:-4])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}